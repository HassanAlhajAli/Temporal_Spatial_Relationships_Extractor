{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanAlhajAli/Temporal_Spatial_Relationships_Extractor/blob/main/arabic_biographies_temporal_spatial_relationships_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsu7-XGq_9vm"
      },
      "source": [
        "# **Create Initial Templates:**\n",
        "\n",
        "\n",
        "\n",
        "* #### تمَّ تصميم مجموعة من القوالب التي يمكن استخدامها لتمثيل العلاقات الزمنية والمكانية بين المفاهيم في مجال الأحداث الشخصية\n",
        "* #### تم تحديد مجموعة من القوالب الأساسية التي يستخدمها المتحدثون لغويًا في تمثيل العلاقات الزمنية والمكانية المحددة، وهي القوالب التي تخص العلاقات الأربعة\n",
        "التي سيتم استخراجها وهي مكان الولادة ومكان الوفاة وتاريخ الولادة وتاريخ الوفاة\n",
        "--------------------------------------------------------------------------------\n",
        "* #### A set of templates were designed that can be used to represent temporal and spatial relationships between concepts in the field of personal events.\n",
        "* #### A set of basic templates that speakers use linguistically to represent specific temporal and spatial relationships have been identified. These templates concern the four relationships to be extracted: place of birth, place of death, date of birth, and date of death.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJMJ2oKcxrYt"
      },
      "outputs": [],
      "source": [
        "# Define different templates for each type of relationship to be extracted\n",
        "templates = {\n",
        "\n",
        "    # birthplace relationship templates\n",
        "    'birthplace': [\n",
        "        'ينحدر {0} من مدينة {1}',\n",
        "        '{0} من مواليد مدينة {1}',\n",
        "        '{1} هي المدينة التي وُلد فيها {0}',\n",
        "        'رأى {0} النور في مدينة {1}',\n",
        "        'أتى {0} إلى الحياة في مدينة {1}',\n",
        "        'يعود مولد {0} إلى مدينة {1}',\n",
        "        'ولد {0} في مدينة {1}'\n",
        "    ],\n",
        "\n",
        "    # deathplace relationship templates\n",
        "    'deathplace': [\n",
        "        'توفي {0} في مدينة {1}',\n",
        "        '{1} هي مكان وفاة {0}',\n",
        "        'رحل {0} إلى رحمة الله في مدينة {1}',\n",
        "        '{1} هي المدينة التي توفي فيها {0}',\n",
        "        'رحل {0} عن الحياة في مدينة {1}',\n",
        "        'فارق {0} الحياة في مدينة {1}',\n",
        "        'انتقل {0} إلى الدار الآخرة في مدينة {1}'\n",
        "    ],\n",
        "\n",
        "    # birthdate relationship templates\n",
        "    'birthdate': [\n",
        "        'ولد {0} في العام {1}',\n",
        "        '{0} من مواليد عام {1}',\n",
        "        '{1} هو عام ولادة {0}',\n",
        "        'يعود تاريخ ولادة {0} إلى العام {1}',\n",
        "        'أتى {0} إلى الحياة عام {1}',\n",
        "        'رأى {0} النور عام {1}',\n",
        "        '{0} فتح عينيه لأول مرة في عام {1}'\n",
        "    ],\n",
        "\n",
        "    # deathdate relationship templates\n",
        "    'deathdate': [\n",
        "        'توفي {0} في العام {1}',\n",
        "        'رحل {0} إلى ربه في عام {1}',\n",
        "        '{1} هو عام وفاة {0}',\n",
        "        'يعود تاريخ وفاة {0} إلى العام {1}',\n",
        "        'رحل {0} إلى رحمة الله عام {1}',\n",
        "        'رحل {0} عن الحياة عام {1}',\n",
        "        'فارق {0} الحياة عام {1}'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06sQ0VYqAaNJ"
      },
      "source": [
        "#**Extending Templates:**\n",
        "* #### تم  توسيع القوالب الموجودة في السؤال الأول عن طريق إيجاد مرادفات لبعض الكلمات المستخدمة في القوالب\n",
        "* #### تم إنشاء قاموس جديد يسمى `replacements` يحتوي على مفاتيح تمثل الكلمات التي يمكن استبدالها، وقيم كل مفتاح هي قائمة بالمرادفات أو الاستبدالات الممكنة لهذه الكلمة\n",
        "#### على سبيل المثال، يمكن استبدال الكلمة \"عام\" بـ \"سنة\" أو \"تاريخ\".\n",
        "\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "* #### The existing templates from the first task were extended by finding synonyms for some of the words used in the templates.\n",
        "* #### A new dictionary called `replacements` was created, containing keys representing the words that can be replaced, and each key's value is a list of synonyms or possible replacements for that word.\n",
        "#### For example, the word \"عام\" (year) can be replaced with \"سنة\" (year) or \"تاريخ\" (date)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItMPtiQ9AZ1H"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary that contains a set of words and their alternative forms\n",
        "replacements = {\n",
        "    'عام': ['سنة', 'تاريخ'],\n",
        "    'مدينة': ['دولة'],\n",
        "    'توفي': ['مات', 'قبض'],\n",
        "    'رأى': ['أبصر', 'شاهد' , 'بصر'],\n",
        "    'رحل': ['ذهب', 'انتقل' , 'ارتحل'],\n",
        "    'أتى': ['أقبل', 'جاء' , 'حضر'],\n",
        "    'الحياة': ['الدنيا', 'العالم'],\n",
        "    'وفاة': ['موت', 'رحبل' , 'منية'],\n",
        "    'مكان': ['موضع', 'منطقة'],\n",
        "    'الله': ['ربه', 'الرحمن' , 'خالقه'],\n",
        "    'فارق': ['غادر', 'ترك']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HVzg3K5xzm5",
        "outputId": "7650ae2e-86b5-46e0-8a4b-fde64ccf83f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of birthplace templates after extending: 22\n",
            "Number of deathplace templates after extending: 37\n",
            "Number of birthdate templates after extending: 29\n",
            "Number of deathdate templates after extending: 47\n"
          ]
        }
      ],
      "source": [
        "# Create a deep copy of the original templates dictionary\n",
        "import copy\n",
        "ext_templates = copy.deepcopy(templates)\n",
        "\n",
        "# Iterate over the original templates and extend them with alternative forms\n",
        "for template_type, template_dict in templates.items():\n",
        "    for template in templates[template_type]:\n",
        "        # Iterate over each word and its alternative forms in the replacements dictionary\n",
        "        for word, replacements_list in replacements.items():\n",
        "            if word in template:\n",
        "                for replacement in replacements_list:\n",
        "                    new_template = template.replace(word, replacement)\n",
        "                    # Add the new template to the extended templates dictionary for the same type of relationship\n",
        "                    ext_templates[template_type].append(new_template)\n",
        "\n",
        "    # Print the number of templates for each type of relationship after they have been extended\n",
        "    print(f\"Number of {template_type} templates after extending: {len(ext_templates[template_type])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Ei-Pm-AnQ-"
      },
      "source": [
        "# **Get Data**\n",
        "\n",
        "* #### تم جمع البيانات التي تتحدث عن مكان الولادة، مكان الإقامة الحالي، تاريخ الولادة، وتاريخ الوفاة إن وجد. لقد قمت ببناء زاحف إلكتروني لاستخراج هذه البيانات من المواقع ذات الصلة، مثل السيرة الذاتية.\n",
        "\n",
        "* #### بسبب أن المواقع باللغة العربية فقيرة بالبيانات أو الصفحات المختصة في هذا المجال فلم يتم جمع الكثير من البيانات في حين أنه في اللغة الإنجليزية هناك مواقع خاصة يمكن استخراج منها آلاف الجمل المتعلقة بهذا المشروع، لذا تم بعد ذلك اللجوء إلى طريقة ثانية في جمع البيانات.\n",
        "\n",
        "* #### الطريق البديلة لقلة البيانات بالزاحف هي توليد البيانات من خلال مصفوفة تحتوي على المدن والدول والأسماء، بالاستناد إلى القوالب التي تم توسيعها.\n",
        "--------------------------------------------------------------------------------\n",
        "* #### Data were collected regarding place of birth, current residence, date of birth, and date of death, if available. A web crawler was built to extract this data from relevant sites, such as biographies.\n",
        "* #### Because Arabic-language websites are poor in data or specialized pages in this field, not much data was collected, while in English there are specialized sites from which thousands of sentences related to this project can be extracted. Therefore, a second method was resorted to for data collection.\n",
        "* #### The alternative method for the lack of data with the crawler is to generate data through a matrix containing cities, countries, and names, based on the expanded templates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCb-OfUB8EQ"
      },
      "source": [
        "## *Generating via Crawler:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccs4gKMKwVjI"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from urllib.parse import urljoin\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Define an empty list to store extracted sentences\n",
        "Data = []\n",
        "\n",
        "# Define a class for the web crawler\n",
        "class Crawler:\n",
        "\n",
        "    def __init__(self, urls=[]):\n",
        "        # Initialize instance variables\n",
        "        self.visited_urls = []\n",
        "        self.urls_to_visit = urls\n",
        "        self.sentences = []\n",
        "        self.limit = 1000\n",
        "\n",
        "    # Download the HTML content of the given URL and return the text\n",
        "    def download_url(self, url):\n",
        "        return requests.get(url).text\n",
        "\n",
        "    def get_sentences(self, html):\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        # Extract all <p> elements\n",
        "        for p in soup.find_all('p'):\n",
        "            # Get the text content of the <p> element and remove leading/trailing whitespace\n",
        "            text = p.get_text().strip()\n",
        "            # Use regex to find sentences that contain certain keywords and add them to the sentences list\n",
        "            new_sentences = re.findall(r'[A-Z][^.!?]*(?:ولد|توفي|مات|تُوُفِيَ|مواليد|ولادة|\\bرحل عن الحياة\\b|وفاة)[^.!?]*[.!?]', text)\n",
        "            for sentence in new_sentences:\n",
        "                self.sentences.append(sentence.strip())\n",
        "                Data.append(sentence.strip())\n",
        "                # Print the extracted sentence\n",
        "                print(f\"Found sentence {len(self.sentences)+1}: {sentence.strip()}\")\n",
        "                # Stop extracting sentences if the limit has been reached\n",
        "                if len(self.sentences) >= self.limit:\n",
        "                    return\n",
        "\n",
        "    def get_linked_urls(self, url, html):\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        # Extract all <a> elements and yield URLs found in the href attribute\n",
        "        for link in soup.find_all('a'):\n",
        "            path = link.get('href')\n",
        "            if path and path.startswith('/'):\n",
        "                path = urljoin(url, path)\n",
        "                yield path\n",
        "\n",
        "    def add_url_to_visit(self, url):\n",
        "        # Add the URL to the urls_to_visit list if it has not already been visited or added\n",
        "        if url not in self.visited_urls and url not in self.urls_to_visit:\n",
        "            self.urls_to_visit.append(url)\n",
        "\n",
        "    def crawl(self, url):\n",
        "        # Download the HTML content of the URL and extract sentences\n",
        "        html = self.download_url(url)\n",
        "        self.get_sentences(html)\n",
        "        # Add any linked URLs to the urls_to_visit list\n",
        "        for url in self.get_linked_urls(url, html):\n",
        "            self.add_url_to_visit(url)\n",
        "\n",
        "    def run(self):\n",
        "        # Loop over urls_to_visit list and crawl each URL until the limit is reached or there are no more URLs to visit\n",
        "        while self.urls_to_visit:\n",
        "            url = self.urls_to_visit.pop(0)\n",
        "            print(f'Crawling: {url}')\n",
        "            try:\n",
        "                self.crawl(url)\n",
        "            except Exception:\n",
        "                print(f'Failed to crawl: {url}')\n",
        "            finally:\n",
        "                # Add the URL to the visited_urls list and stop extracting sentences if the limit has been reached\n",
        "                self.visited_urls.append(url)\n",
        "                if len(self.sentences) >= self.limit:\n",
        "                    break\n",
        "        # Print the number of extracted sentences\n",
        "        print(f'Found {len(self.sentences)} sentences ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hpaFCM-PwVfd",
        "outputId": "64ead997-baae-4d9e-d11d-ce4a71b884c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://mawdoo3.com/%D8%A3%D9%87%D9%85_%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1\n",
            "Crawling: https://mawdoo3.com/#categories\n",
            "Crawling: https://mawdoo3.com/خاص:أجدد_الصفحات\n",
            "Crawling: https://mawdoo3.com/خاص:الصفحات_الأكثر_مشاهدة\n",
            "Crawling: https://mawdoo3.com/\n",
            "Crawling: https://mawdoo3.com/تصنيف:علماء\n",
            "Crawling: https://mawdoo3.com/فريق_موضوع\n",
            "Crawling: https://mawdoo3.com/أشهر_العلماء_المسلمين\n",
            "Crawling: https://mawdoo3.com/أهم_العلماء_في_العصر_العباسي\n",
            "Crawling: https://mawdoo3.com/أهم_العلماء_العرب\n",
            "Crawling: https://mawdoo3.com/علماء_الفلك_في_العصر_العباسي\n",
            "Crawling: https://mawdoo3.com/%D8%A3%D8%B4%D9%87%D8%B1_%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1_%D8%A7%D9%84%D9%85%D8%B3%D9%84%D9%85%D9%8A%D9%86\n",
            "Crawling: https://mawdoo3.com/%D8%A3%D9%87%D9%85_%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1_%D9%81%D9%8A_%D8%A7%D9%84%D8%B9%D8%B5%D8%B1_%D8%A7%D9%84%D8%B9%D8%A8%D8%A7%D8%B3%D9%8A\n",
            "Crawling: https://mawdoo3.com/أهم_علماء_الرياضيات\n",
            "Crawling: https://mawdoo3.com/أشهر_علماء_الأندلس_المسلمين\n",
            "Crawling: https://mawdoo3.com/أهم_إنجازات_علماء_المسلمين_في_الطب\n",
            "Crawling: https://mawdoo3.com/أهم_علماء_الحديث\n",
            "Crawling: https://mawdoo3.com/أهم_الفلاسفة_المسلمين\n",
            "Crawling: https://mawdoo3.com/أبو_بكر_الرازي\n",
            "Crawling: https://mawdoo3.com/من_هو_نوبل\n",
            "Crawling: https://mawdoo3.com/من_هو_مخترع_التليفون\n",
            "Crawling: https://mawdoo3.com/جابر_بن_حيان\n",
            "Crawling: https://mawdoo3.com/من_هو_جابر_بن_حيان\n",
            "Crawling: https://mawdoo3.com/أول_من_اخترع_الكاميرا\n",
            "Found sentence 2: Louis-Jacques-Mandé Daguerre) هو فيزيائي ورسّام فرنسي وُلد عام 1787م بالقرب من باريس في فرنسا، وهو صاحب اختراع أول عملية تصوير فوتوغرافي والتي عُرفت بإسم الداجيروتايب (بالإنجليزية: Daguerreotype)، وتوفيّ لويس في عام 1851م.\n",
            "Crawling: https://mawdoo3.com/العالم_شرودنجر\n",
            "Crawling: https://mawdoo3.com/اسم_عالم_رياضيات\n",
            "Crawling: https://mawdoo3.com/من_هو_مخترع_الحاسوب\n",
            "Found sentence 3: Charles Babbage) أول من ساهم في اختراع أول أجهزة الحاسوب الآليّة التي وضعت الأساس لتصميم أجهزة الحاسوب الأكثر تعقيداً، وقد وُلد بابيج في مدينة لندن في إنجلترا، في السادس والعشرين من شهر كانون الأول عام 1791م، ويُطلق عليه لقب \"أبو الحوسبة\"، إذ وضع خُططاً تفصيليّةً لحساب محركات الحسابات الميكانيكيّة، ومحركات الفرق، والمحركات التحليليّة، وتوفي في مسقط رأسه، في تاريخ الثامن عشر من شهر تشرين الأول عام 1871م.\n",
            "Found sentence 4: The Analytical Engine) خطوة كبيرة بعد مُحرك الفرق، حيث عمل بابيج على اختراع جهاز يُمكنه من حل كافة العمليّات الحسابيّة، والعديد من المشكلات المُتنوعة؛ كالمشكلات الرقميّة، والأوتوماتيكيّة، والميكانيكيّة، بحيث يُمكن التحكم بها من خلال برامج مُتعددة، وقد اشتمل المحرك التحليليّ على أربعة أجزاء رئيسيّة، وهي: وحدة المعالجة المركزيّة، وهي مكان إجراء الحسابات، والذاكرة، وهي مكان حفظ المعلومات المُسجلة، ولوحة المفاتيح، التي تسمح بإدخال البيانات باستخدام البطاقات المثقوبة، بالإضافة إلى الطابعة.\n",
            "Crawling: https://mawdoo3.com/العالم_فونت\n",
            "Crawling: https://mawdoo3.com/من_هو_مخترع_المدرسة؟\n",
            "Found sentence 5: Horace Mann) الملقَّب بأب حركة المدرسة المُشتركَة هو معلِّم أمريكي ولد في فرانكلين، في ولاية ماساتشوستس في الولايات المتحدة الأمريكية عام 1796م نشأ في حالةٍ من الفقر والمشقة وإنكار الذات ومع ذلك أكمل تعليمه، ويعدّ أول مدافع عن التعليم العام، والذي كان يعتقد أنه يجب أن يكون حرًا وعالميًا وعلى يد معلمين متدرّبين محترفين.\n",
            "Crawling: https://mawdoo3.com/العالم_نيوتن\n",
            "Crawling: https://mawdoo3.com/العالم_فاراداي\n",
            "Found sentence 6: Michael Faraday) عالم فيزيائي وكيميائي إنجليزي ساهم في القيام بالعديد من التجارب التي ساهمت في فهم الكهرومغناطيسية، وقد ولد في تاريخ 22 أيلول 1791م في إنجلترا وتوفي في تاريخ 25 آب 1867م في هامبتون كورت في سري، ويعتبر فارادي واحداً من أعظم علماء القرن التاسع عشر، وذلك بسبب حياته المهنية ككيميائي، بالإضافة إلى كتابته لدليلاً حول الكيمياء العملية واكتشافه لعدداً من المركبات العضوية الجديدة من بينها البنزين، كما ويعتبر أول من استطاع أن يسيل غازاً، ولكن مساهمته الرئيسية هي في مجال الكهرباء والمغناطيسية، كونه يعتبر أول من أنتج تياراً كهربائياً من مجال مغناطيسي، وهو أول من اخترع أول محرك كهربائي ودينمو وأظهر العلاقة ما بين الكهرباء والكيميائية الترابطية، واكتشف تأثير المغناطيسية على الضوء.\n",
            "Crawling: https://mawdoo3.com/معلومات_عن_نيوتن\n",
            "Crawling: https://mawdoo3.com/أبو_القاسم_الزهراوي\n",
            "Crawling: https://mawdoo3.com/ابن_باجة\n",
            "Crawling: https://mawdoo3.com/بحث_عن_أينشتاين\n",
            "Crawling: https://mawdoo3.com/بحث_عن_نيوتن\n",
            "Crawling: https://mawdoo3.com/دكتور_زغلول_النجار\n",
            "Crawling: https://mawdoo3.com/أبقراط\n",
            "Crawling: https://mawdoo3.com/تلاميذ_أبو_الأسود_الدؤلي\n",
            "Crawling: https://mawdoo3.com/الأخلاق_عند_وليم_جيمس\n",
            "Crawling: https://mawdoo3.com/ما_هي_سرعة_الصوت\n",
            "Crawling: https://mawdoo3.com/ما_هو_الدين_الاسلامي\n",
            "Crawling: https://mawdoo3.com/طريقة_الخبز_بالثوم\n",
            "Crawling: https://mawdoo3.com/ما_هو_لون_البراز_الطبيعي\n",
            "Crawling: https://mawdoo3.com/أين_يقع_الأخدود\n",
            "Crawling: https://mawdoo3.com/طرق_تنظيف_الخشب\n",
            "Crawling: https://mawdoo3.com/طريقة_عمل_عرق_الفلتو_في_الفرن\n",
            "Crawling: https://mawdoo3.com/شروط_التوبة_من_الزنا\n",
            "Crawling: https://mawdoo3.com/معلومات_غريبة_عن_الفضاء\n",
            "Crawling: https://mawdoo3.com/تفسير_سورة_الانفطار\n",
            "Crawling: https://mawdoo3.com/اتصل_بنا\n",
            "Crawling: https://mawdoo3.com/اتفاقية_الاستخدام\n",
            "Found sentence 7: Google Analytics): ان الموقع \n",
            "يستخدم الكوكيز الخاصة يه وتقنيات مماثلة لحفظ بعض المعلومات وتعقب البيانات وعدد الزوار والمعلومات الخاصة بهم بالاضافة الى استخدام الكوكيز الخاصة بأطراف ثالثة كالمعلنين أو وكالات الإعلان الذين يضعون إعلانات على الموقع.\n",
            "Found sentence 8: Google Analytics الـ \"كوكيز\" لتجميع وتحليل المعلومات حول طريقة استخدام الموقع .\n",
            "Crawling: https://mawdoo3.com/About_Us\n",
            "Crawling: https://mawdoo3.com/عن_موضوع\n",
            "Found sentence 9: SEO) لموقعك الإلكتروني، بالإضافة إلى خدمات المحتوى الرقمي، ما يساهم في تعزيز علامتك التجارية على الإنترنت وزيادة عدد الزوّار لموقعك الخاصّ.\n",
            "Crawling: https://mawdoo3.com/سياسة_الخصوصية\n",
            "Crawling: https://mawdoo3.com/#categories\n",
            "Crawling: https://mawdoo3.com/معاييرنا_للتدقيق\n",
            "Crawling: https://mawdoo3.com/مدن_سياحية_في_السعودية\n",
            "Crawling: https://mawdoo3.com/تصنيف:مدن_عربية\n",
            "Crawling: https://mawdoo3.com/لماذا_سميت_الأشهر_الحرم_بهذا_الاسم\n",
            "Crawling: https://mawdoo3.com/تصنيف:معلومات_إسلامية\n",
            "Crawling: https://mawdoo3.com/تكبيرات_الحج\n",
            "Crawling: https://mawdoo3.com/تصنيف:إسلام\n",
            "Crawling: https://mawdoo3.com/أحكام_الأضحية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أحكام_شرعية\n",
            "Crawling: https://mawdoo3.com/أسهل_طريقة_لعمل_كعك_العيد\n",
            "Crawling: https://mawdoo3.com/تصنيف:الكعك_والمعمول\n",
            "Crawling: https://mawdoo3.com/أجمل_مدن_تركيا_في_الصيف\n",
            "Crawling: https://mawdoo3.com/تصنيف:سياحة\n",
            "Crawling: https://mawdoo3.com/الفرق_بين_الذهب_الأبيض_والفضة\n",
            "Crawling: https://mawdoo3.com/تصنيف:تعليم\n",
            "Crawling: https://mawdoo3.com/في_أي_شهر_يبدأ_الطفل_بالأكل\n",
            "Crawling: https://mawdoo3.com/تصنيف:كيف_أتعامل_مع_طفلي\n",
            "Crawling: https://mawdoo3.com/كيف_تعمل_السيارات_الكهربائية\n",
            "Crawling: https://mawdoo3.com/تصنيف:وسائل_النقل\n",
            "Crawling: https://mawdoo3.com/تصنيف:فن_الطهي\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_رئيسية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_جانبية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_شرقية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_شامية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_خليجية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_صحية\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_الأرز\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_الدجاج\n",
            "Crawling: https://mawdoo3.com/تصنيف:أطباق_اللحوم\n",
            "Crawling: https://mawdoo3.com/تصنيف:طبخ_عربي\n",
            "Crawling: https://mawdoo3.com/تصنيف:أكلات_سريعة\n",
            "Crawling: https://mawdoo3.com/تصنيف:أكلات_خفيفة\n",
            "Crawling: https://mawdoo3.com/تصنيف:أكلات_منوعة\n",
            "Crawling: https://mawdoo3.com/تصنيف:مأكولات_بحرية\n",
            "Crawling: https://mawdoo3.com/تصنيف:المعكرونة\n",
            "Crawling: https://mawdoo3.com/تصنيف:المحاشي\n",
            "Crawling: https://mawdoo3.com/تصنيف:مقبلات_و_سلطات\n",
            "Crawling: https://mawdoo3.com/تصنيف:سلطات\n",
            "Crawling: https://mawdoo3.com/تصنيف:عجائن_ومخبوزات\n",
            "Crawling: https://mawdoo3.com/تصنيف:شوربات\n",
            "Crawling: https://mawdoo3.com/تصنيف:المخللات\n",
            "Crawling: https://mawdoo3.com/تصنيف:أساسيات_فن_الطهي\n",
            "Crawling: https://mawdoo3.com/تصنيف:حفظ_الأطعمة\n",
            "Crawling: https://mawdoo3.com/تصنيف:تزيين_اﻷطباق\n",
            "Crawling: https://mawdoo3.com/تصنيف:البهارات\n",
            "Crawling: https://mawdoo3.com/تصنيف:صلصات\n",
            "Crawling: https://mawdoo3.com/تصنيف:الشوفان\n",
            "Crawling: https://mawdoo3.com/تصنيف:مشروبات_باردة_و_ساخنة\n",
            "Crawling: https://mawdoo3.com/تصنيف:عصائر\n",
            "Crawling: https://mawdoo3.com/تصنيف:حلو_عالمي\n",
            "Crawling: https://mawdoo3.com/تصنيف:حلو_عربي\n",
            "Crawling: https://mawdoo3.com/تصنيف:حلويات_رمضان\n",
            "Crawling: https://mawdoo3.com/تصنيف:حلويات_باردة\n",
            "Crawling: https://mawdoo3.com/تصنيف:حلويات_القطر\n",
            "Crawling: https://mawdoo3.com/تصنيف:كيك\n",
            "Crawling: https://mawdoo3.com/تصنيف:البان_كيك_والكريب\n",
            "Crawling: https://mawdoo3.com/تصنيف:منوعات_في_فن_الطهي\n",
            "Crawling: https://mawdoo3.com/تصنيف:حول_العالم\n",
            "Crawling: https://mawdoo3.com/تصنيف:مدن_وبلدان\n",
            "Crawling: https://mawdoo3.com/تصنيف:مدن_أجنبية\n",
            "Crawling: https://mawdoo3.com/تصنيف:مدن_ومحافظات\n",
            "Crawling: https://mawdoo3.com/تصنيف:عواصم\n",
            "Crawling: https://mawdoo3.com/تصنيف:دول_أجنبية\n",
            "Crawling: https://mawdoo3.com/تصنيف:دول_عربية\n",
            "Crawling: https://mawdoo3.com/تصنيف:دول_قارة_آسيا\n",
            "Crawling: https://mawdoo3.com/تصنيف:معالم_سياحية\n",
            "Crawling: https://mawdoo3.com/تصنيف:معالم_وآثار\n",
            "Crawling: https://mawdoo3.com/تصنيف:الكثافة_السكانية\n",
            "Crawling: https://mawdoo3.com/تصنيف:مساحات_الدول\n",
            "Crawling: https://mawdoo3.com/تصنيف:أنظمة_دولية\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالذات\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالجسم\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالقدم\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_باليدين\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالأظافر\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالفم_و_الأسنان\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالوجه\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالبشرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:صحة_البشرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:تنظيف_وتقشير_البشرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:تفتيح_لون_البشرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:الهالات_والرؤوس_السوداء\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الزيوت_للبشرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:حبوب_البشرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالشعر\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بفروة_الرأس\n",
            "Crawling: https://mawdoo3.com/تصنيف:العناية_بالشعر_التالف\n",
            "Crawling: https://mawdoo3.com/تصنيف:صبغات_الشعر\n",
            "Crawling: https://mawdoo3.com/تصنيف:صحة_الشعر\n",
            "Crawling: https://mawdoo3.com/تصنيف:وصفات_تكثيف_الشعر\n",
            "Crawling: https://mawdoo3.com/تصنيف:وصفات_تطويل_الشعر\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الزيوت_للشعر\n",
            "Crawling: https://mawdoo3.com/تصنيف:تجميل_ومكياج\n",
            "Crawling: https://mawdoo3.com/تصنيف:جمال\n",
            "Crawling: https://mawdoo3.com/تصنيف:حليّ_ومجوهرات\n",
            "Crawling: https://mawdoo3.com/تصنيف:أزياء_وملابس\n",
            "Crawling: https://mawdoo3.com/تصنيف:حب_الشباب\n",
            "Crawling: https://mawdoo3.com/تصنيف:منوعات_في_العناية_بالذات\n",
            "Crawling: https://mawdoo3.com/تصنيف:منوعات_في_الجمال_والأناقة\n",
            "Crawling: https://mawdoo3.com/تصنيف:مال_وأعمال\n",
            "Crawling: https://mawdoo3.com/تصنيف:اقتصاد_مالي\n",
            "Crawling: https://mawdoo3.com/تصنيف:عملات\n",
            "Crawling: https://mawdoo3.com/تصنيف:صناعات\n",
            "Crawling: https://mawdoo3.com/تصنيف:عقارات\n",
            "Crawling: https://mawdoo3.com/تصنيف:سؤال_وجواب\n",
            "Crawling: https://mawdoo3.com/تصنيف:كيف_أهتم_بصحة_بشرتي\n",
            "Crawling: https://mawdoo3.com/تصنيف:كيف_أهتم_بصحة_شعري\n",
            "Crawling: https://mawdoo3.com/تصنيف:كيف_أطور_مهاراتي\n",
            "Crawling: https://mawdoo3.com/تصنيف:تقنية\n",
            "Crawling: https://mawdoo3.com/تصنيف:كمبيوتر\n",
            "Crawling: https://mawdoo3.com/تصنيف:موبايل\n",
            "Crawling: https://mawdoo3.com/تصنيف:تقنيات_منوعة\n",
            "Crawling: https://mawdoo3.com/تصنيف:انترنت\n",
            "Crawling: https://mawdoo3.com/تصنيف:مواقع_التواصل_اﻹجتماعي\n",
            "Crawling: https://mawdoo3.com/تصنيف:برمجة_وتصميم_المواقع\n",
            "Crawling: https://mawdoo3.com/تصنيف:تطبيقات_الكترونية\n",
            "Crawling: https://mawdoo3.com/تصنيف:برامج_الكترونية\n",
            "Crawling: https://mawdoo3.com/تصنيف:اسئلة_تقنية\n",
            "Crawling: https://mawdoo3.com/تصنيف:منوعات_تقنية\n",
            "Crawling: https://mawdoo3.com/تصنيف:علوم_الأرض\n",
            "Crawling: https://mawdoo3.com/تصنيف:بحار_ومحيطات\n",
            "Crawling: https://mawdoo3.com/تصنيف:أنهار_وبحيرات\n",
            "Crawling: https://mawdoo3.com/تصنيف:جبال_ووديان\n",
            "Crawling: https://mawdoo3.com/تصنيف:زراعة_الخضراوات_والفواكه\n",
            "Crawling: https://mawdoo3.com/تصنيف:زراعة\n",
            "Crawling: https://mawdoo3.com/تصنيف:ظواهر_طبيعية\n",
            "Crawling: https://mawdoo3.com/تصنيف:ثروات_طبيعية\n",
            "Crawling: https://mawdoo3.com/تصنيف:التلوث_البيئي\n",
            "Crawling: https://mawdoo3.com/تصنيف:المجموعة_الشمسية\n",
            "Crawling: https://mawdoo3.com/تصنيف:منوعات_عن_الطبيعة\n",
            "Crawling: https://mawdoo3.com/تصنيف:فنون\n",
            "Crawling: https://mawdoo3.com/تصنيف:فنون_منوعة\n",
            "Crawling: https://mawdoo3.com/تصنيف:منوعات_فنية\n",
            "Crawling: https://mawdoo3.com/تصنيف:قصص_وحكايات\n",
            "Crawling: https://mawdoo3.com/تصنيف:قصص_عالمية\n",
            "Crawling: https://mawdoo3.com/تصنيف:قصص_عربية\n",
            "Crawling: https://mawdoo3.com/تصنيف:قصص_دينية\n",
            "Crawling: https://mawdoo3.com/تصنيف:تغذية\n",
            "Crawling: https://mawdoo3.com/تصنيف:فيتامينات_ومعادن\n",
            "Crawling: https://mawdoo3.com/تصنيف:مصادر_الفيتامينات_والمعادن\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الفيتامينات_والمعادن\n",
            "Crawling: https://mawdoo3.com/تصنيف:نقص_الفيتامينات_والمعادن\n",
            "Crawling: https://mawdoo3.com/تصنيف:الفاكهة_والخضراوات\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الفواكه\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الخضروات\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الأعشاب\n",
            "Crawling: https://mawdoo3.com/تصنيف:التداوي_باﻷعشاب\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_الحبوب\n",
            "Crawling: https://mawdoo3.com/تصنيف:فوائد_البذور\n",
            "Crawling: https://mawdoo3.com/تصنيف:معلومات_غذائية\n",
            "Crawling: https://mawdoo3.com/تصنيف:منتجات_غذائية\n",
            "Crawling: https://mawdoo3.com/تصنيف:حليب_وأجبان\n",
            "Crawling: https://mawdoo3.com/تصنيف:الشاي_والقهوة\n",
            "Crawling: https://mawdoo3.com/تصنيف:ماء_الورد\n",
            "Crawling: https://mawdoo3.com/تصنيف:العسل\n",
            "Crawling: https://mawdoo3.com/تصنيف:قرآن\n",
            "Crawling: https://mawdoo3.com/تصنيف:أحاديث\n",
            "Crawling: https://mawdoo3.com/تصنيف:تأملات_قرآنية\n",
            "Crawling: https://mawdoo3.com/تصنيف:عبادات\n",
            "Crawling: https://mawdoo3.com/تصنيف:وضوء_و_صلاة\n",
            "Crawling: https://mawdoo3.com/تصنيف:وضوء_وطهارة\n",
            "Crawling: https://mawdoo3.com/تصنيف:فروض_وسنن\n",
            "Crawling: https://mawdoo3.com/تصنيف:النوافل\n",
            "Crawling: https://mawdoo3.com/تصنيف:الحج_والعمرة\n",
            "Crawling: https://mawdoo3.com/تصنيف:أذكار\n",
            "Crawling: https://mawdoo3.com/تصنيف:شهر_رمضان\n",
            "Crawling: https://mawdoo3.com/تصنيف:دعاء_1\n",
            "Crawling: https://mawdoo3.com/تصنيف:دعاء_2\n",
            "Crawling: https://mawdoo3.com/تصنيف:دعاء_3\n",
            "Crawling: https://mawdoo3.com/تصنيف:دعاء_4\n",
            "Crawling: https://mawdoo3.com/تصنيف:حساب_الزكاة\n",
            "Crawling: https://mawdoo3.com/تصنيف:التاريخ_اﻹسلامي\n",
            "Crawling: https://mawdoo3.com/تصنيف:معارك_و_غزوات\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f239706604e2>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Create a new instance of the Crawler class with a single URL in its urls list, and call its run method to begin crawling the specified URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mCrawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-bce0750c2635>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Crawling: {url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Failed to crawl: {url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bce0750c2635>\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Download the HTML content of the URL and extract sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Add any linked URLs to the urls_to_visit list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_linked_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bce0750c2635>\u001b[0m in \u001b[0;36mget_sentences\u001b[0;34m(self, html)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Parse the HTML content using BeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Extract all <p> elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<!--\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/html/parser.py\u001b[0m in \u001b[0;36mparse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgtpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cdata_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgtpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mhandle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malready_closed_empty_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mhandle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \"\"\"\n\u001b[1;32m    769\u001b[0m         \u001b[0;31m#print(\"End tag: \" + name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popToTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mendData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \"\"\"       \n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mcurrent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;31m# If whitespace is not preserved, and this string contains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "URL = 'https://arab-ency.com/'\n",
        "URL = 'https://ar.wikipedia.org/wiki/%D8%B3%D9%8A%D8%B1%D8%A9_%D8%B0%D8%A7%D8%AA%D9%8A%D8%A9'\n",
        "# URL = 'https://www.biography.com/'\n",
        "# URL = 'https://www.baynoona.net/ar/ebook/340'\n",
        "# URL = 'https://www.lahamag.com/category/70-%D8%A3%D8%AE%D8%A8%D8%A7%D8%B1-%D8%A7%D9%84%D9%86%D8%AC%D9%88%D9%85'\n",
        "# URL = 'https://www.elfann.com/'\n",
        "# URL = 'https://www.sayidaty.net/category/main/%D9%85%D8%B4%D8%A7%D9%87%D9%8A%D8%B1/%D9%85%D8%B4%D8%A7%D9%87%D9%8A%D8%B1-%D8%A7%D9%84%D8%B9%D8%B1%D8%A8'\n",
        "# URL = 'https://www.mosaiquefm.net/ar/actualites/%D8%A3%D8%AE%D8%A8%D8%A7%D8%B1-%D8%A7%D9%84%D9%85%D8%B4%D8%A7%D9%87%D9%8A%D8%B1/38'\n",
        "# URL = 'https://www.layalina.com/all-celebrity-news/'\n",
        "# URL = 'https://arabic.euronews.com/tag/celebrity-news'\n",
        "# URL = 'https://www.filfan.com/sections/3'\n",
        "# URL = 'https://quranwahadith.com/%D8%A7%D9%84%D9%83%D8%AA%D8%A8-%D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A%D8%A9/%D8%B3%D9%8A%D8%B1%D8%A9/%D8%AD%D9%8A%D8%A7%D8%A9-%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1/'\n",
        "# URL = 'https://www.twinkl.com/teaching-wiki/lma-alrb\n",
        "# URL = 'https://www.aljazeera.net/blogs/2019/9/24/%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1-%D8%A7%D9%84%D9%85%D8%B3%D9%84%D9%85%D9%88%D9%86-%D9%88%D9%81%D8%B6%D9%84%D9%87%D9%85-%D8%A7%D9%84%D8%B0%D9%8A-%D8%BA%D9%8A%D8%A8-%D8%B9%D9%86%D8%A7'\n",
        "# URL = 'https://www.aljazeera.net/blogs/2019/9/24/%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1-%D8%A7%D9%84%D9%85%D8%B3%D9%84%D9%85%D9%88%D9%86-%D9%88%D9%81%D8%B6%D9%84%D9%87%D9%85-%D8%A7%D9%84%D8%B0%D9%8A-%D8%BA%D9%8A%D8%A8-%D8%B9%D9%86%D8%A7'\n",
        "URL = 'https://mawdoo3.com/%D8%A3%D9%87%D9%85_%D8%A7%D9%84%D8%B9%D9%84%D9%85%D8%A7%D8%A1'\n",
        "# URL = 'https://www.marefa.org/%D9%82%D8%A7%D8%A6%D9%85%D8%A9_%D8%A7%D9%84%D9%83%D8%AA%D8%A7%D8%A8_%D8%A7%D9%84%D8%B9%D8%B1%D8%A8'\n",
        "# URL = 'https://ar.wikipedia.org/wiki/%D8%AA%D8%B5%D9%86%D9%8A%D9%81:%D8%B9%D9%84%D9%85%D8%A7%D8%A1'\n",
        "\n",
        "# Create a new instance of the Crawler class with a single URL in its urls list, and call its run method to begin crawling the specified URL\n",
        "Crawler(urls=[URL]).run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNWxPBpB4Gt5"
      },
      "outputs": [],
      "source": [
        "print(len(Data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNomRo3cBuSH"
      },
      "source": [
        "## *Random Generating:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUR5RrES2_Ak"
      },
      "outputs": [],
      "source": [
        "# Define a list of names\n",
        "names = ['أحمد', 'علي', 'محمد', 'عمر', 'حسين', 'عبدالله', 'فهد', 'خالد', 'سلمان', 'فيصل',\n",
        "              'ناصر', 'سعد', 'ماجد', 'محمود', 'أسامة', 'حمد', 'منصور', 'مروان', 'شادي', 'عبدالرحيم',\n",
        "              'طارق', 'حاتم', 'سامي', 'يوسف', 'أشرف', 'محمد علي', 'هاني', 'رائد', 'عادل', 'بدر',\n",
        "              'حسن', 'محمد مصطفى', 'زيد', 'أحمد علي', 'محمدعبدالرحمن', 'رشاد', 'عماد', 'باسل', 'وليد', 'عبدالمجيد',\n",
        "              'مصطفى', 'عبدالرحمن', 'سعيد', 'ياسر', 'محمد ناصر', 'رشيد', 'نواف', 'إبراهيم', 'سهيل', 'حسام']\n",
        "\n",
        "#check frequncies:\n",
        "# for x in names:\n",
        "#   print(x , \" : \",  names.count(x))\n",
        "# print(len(names))\n",
        "\n",
        "# Define a list of cities\n",
        "cities = ['نيويورك', 'لندن', 'طوكيو', 'باريس', 'برلين', 'مدريد', 'موسكو', 'دبي', 'إسطنبول', 'سنغافورة',\n",
        "          'سدني', 'كندا', 'واشنطن.', 'كاليفورنيا', 'ميلانو', 'روما', 'فيينا', 'حمص', 'أمستردام', 'حلب',\n",
        "          'ساو باولو', 'ريو دي جانيرو', 'اللاذقية', 'مكسيكو', 'بكين', 'شنغهاي', 'هونغ كونغ', 'سيول', 'الحسكة',\n",
        "          'ملبورن', 'بريسبان', 'كوالالمبور', 'بنكوك', 'دلهي', 'مومباي', 'جاكرتا', 'نيو دلهي', 'كراتشي', 'إسلام أباد',\n",
        "          'لاهور', 'طهران', 'بغداد', 'الإسكندرية', 'طرطوس', 'القاهرة', 'الجزائر', 'تونس', 'طرابلس', 'بيروت', 'دمشق']\n",
        "\n",
        "#check frequncies:\n",
        "# for x in cities:\n",
        "#   print(x , \" : \",  cities.count(x))\n",
        "# print(len(cities))\n",
        "\n",
        "# Define a list of countries\n",
        "countries = ['المملكة المتحدة', 'الولايات المتحدة الأمريكية', 'الصين', 'روسيا', 'كندا', 'البرازيل', 'أستراليا', 'الهند',\n",
        "             'فرنسا', 'ألمانيا', 'إيطاليا', 'اليابان', 'كوريا الجنوبية', 'المكسيك', 'إسبانيا', 'إندونيسيا', 'تركيا',\n",
        "             'جنوب أفريقيا', 'نيجيريا', 'مصر', 'الأرجنتين', 'السعودية', 'إيران', 'باكستان', 'بنجلاديش', 'فيتنام',\n",
        "             'كولومبيا', 'بولندا', 'كينيا', 'العراق', 'المغرب', 'السودان', 'أوكرانيا', 'تنزانيا', 'أوغندا', 'كنيا',\n",
        "             'الفلبين', 'الجزائر', 'المملكة العربية السعودية', 'الإمارات العربية المتحدة', 'قطر', 'الكويت', 'عمان',\n",
        "             'الأردن', 'سوريا', 'لبنان', 'فلسطين', 'بلجيكا', 'سويسرا', 'الدنمارك']\n",
        "\n",
        "#check frequncies:\n",
        "# for x in countries:\n",
        "#   print(x , \" : \",  countries.count(x))\n",
        "# print(len(countries))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FtK7yXNAbO1"
      },
      "outputs": [],
      "source": [
        "Data = Data[0:9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFm9GYdR2-1A"
      },
      "outputs": [],
      "source": [
        "# Import the random module\n",
        "import random\n",
        "\n",
        "# Set the data size (number of sentences to generate)\n",
        "Data_size = 1200\n",
        "\n",
        "for i in range(Data_size):\n",
        "    # Loop through each template type and its associated templates in the ext_templates dictionary\n",
        "    for template_type, templates_list in ext_templates.items():\n",
        "        # Choose a random name from the names list, Choose a random template from the templates_list\n",
        "        name = random.choice(names)\n",
        "        template = random.choice(templates_list)\n",
        "\n",
        "        x = 'second parameter'\n",
        "        # If the template contains the word 'مكان', choose a random city or country\n",
        "        if 'مكان' in template or 'منطقة' in template or 'موضع' in template:\n",
        "            x = random.choice(cities + countries)\n",
        "        # If the template contains the word 'مدينة', choose a random city\n",
        "        elif 'مدينة' in template:\n",
        "            x = random.choice(cities)\n",
        "        # If the template contains the word 'دولة', choose a random country\n",
        "        elif 'دولة' in template:\n",
        "            x = random.choice(countries)\n",
        "        # If the template contains none of the above, choose a random integer between 1000 and 2023\n",
        "        else:\n",
        "            x = random.randint(1000, 2023)\n",
        "\n",
        "        sentence = template.format(name, x)\n",
        "        print(sentence)\n",
        "        Data.append(sentence)\n",
        "\n",
        "    # If the desired data size is reached, break out of the loop\n",
        "    if len(Data) >= Data_size:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGTzOonXFkz4"
      },
      "outputs": [],
      "source": [
        "# Loop through each item in the Data list using the range() function\n",
        "for i in range(len(Data)):\n",
        "    # Print the index of the item (i+1 to start with 1 instead of 0), a colon, and the item itself\n",
        "    print(i+1, \" : \", Data[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSRm3ksJ-Ug2"
      },
      "source": [
        "# **Apply Templates Using Farasa and Regular Expressions**\n",
        "\n",
        "* #### تم استخدام مكتبة فراسة لتعرف الكيانات وتحديد المدن والدول (أي المواقع) والأسماء.\n",
        "#### بالإضافة إلى ذلك، تم تحديد التواريخ من خلال الأرقام باستخدام التعابير المنتظمة.\n",
        "* ####  تم تطبيق القوالب على الوثائق المولدة والمجموعة من خلال الزحف الإلكتروني، وذلك باستخدام التعابير المنتظمة.\n",
        "\n",
        "* #### تم تنفيذ النظام على أربع مراحل وهي كالتالي: تم تطبيق النظام على أربعة مراحل وهي كالتالي:\n",
        "#### 1.\tاستخراج علامات الترقيم ما عدا القوس المعقوف من الجمل (لأنه يشمل المتغيرات)\n",
        "#### 2.\tتضمين الكلمات الأساسية في القوالب في الجمل.\n",
        "#### 3.\tالسماح بوجود كلمات زائدة بين الكلمات الأساسية لتحقيق مرونة في تحديد العلاقات.\n",
        "#### 4.\tفي حالة مطابقة القالب للجملة، يتم استخراج العلاقات الزمنية والمكانية\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "* #### The Farasa library was used for named entity recognition to identify cities, countries (i.e., locations), and names.\n",
        "#### Additionally, dates were identified using numbers with regular expressions.\n",
        "* #### The templates were applied to the generated and crawled documents using regular expressions.\n",
        "\n",
        "* #### The system was implemented in four stages as follows:\n",
        "#### 1. Extracting punctuation marks except for curly braces from the sentences (because they include variables).\n",
        "#### 2. Embedding the keywords from the templates into the sentences.\n",
        "#### 3. Allowing extra words between the keywords to provide flexibility in identifying relationships.\n",
        "#### 4. If the template matches the sentence, temporal and spatial relationships are extracted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H76xRs31dAd5"
      },
      "outputs": [],
      "source": [
        "!pip install farasa farasapy\n",
        "\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "from farasa.pos import FarasaPOSTagger\n",
        "from farasa.ner import FarasaNamedEntityRecognizer\n",
        "\n",
        "# Load Farasa tools\n",
        "segmenter = FarasaSegmenter(interactive=True)\n",
        "pos = FarasaPOSTagger(interactive=True)\n",
        "ner = FarasaNamedEntityRecognizer(interactive=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTcetjNamg4a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define a function to extract named entities from a sentence using FarasaNamedEntityRecognizer\n",
        "def get_entities(sentence):\n",
        "    # Use the FarasaNamedEntityRecognizer to identify named entities in the sentence\n",
        "    entities = ner.recognize(sentence)\n",
        "    # Split the entities into a list of tokens\n",
        "    return entities.split()\n",
        "\n",
        "# Define a function to extract the location from a sentence\n",
        "def get_location(sentence):\n",
        "    ent = get_entities(sentence)\n",
        "    # Loop through the entities and look for a 'B-LOC' entity tag\n",
        "    for e in ent:\n",
        "        if 'B-LOC' in e:\n",
        "            return e.split('/')[0]\n",
        "\n",
        "    # If no 'B-LOC' tag is found, look for a city or country name\n",
        "    for e in ent:\n",
        "        if e.split('/')[0] in (cities + countries):\n",
        "            return e.split('/')[0]\n",
        "\n",
        "    # If no location is found, return 'None'\n",
        "    return 'None'\n",
        "\n",
        "# Define a function to extract the person name from a sentence\n",
        "def get_person(sentence):\n",
        "    ent = get_entities(sentence)\n",
        "    # Loop through the entities and look for a 'B-PERS' entity tag\n",
        "    for e in ent:\n",
        "        if 'B-PERS' in e:\n",
        "            return e.split('/')[0]\n",
        "\n",
        "    # If no 'B-PERS' tag is found, look for a name from the names list\n",
        "    for e in ent:\n",
        "        if e.split('/')[0] in names:\n",
        "            return e.split('/')[0]\n",
        "\n",
        "    # If no person name is found, return 'None'\n",
        "    return 'None'\n",
        "\n",
        "# Define a function to extract the date from a sentence using regular expressions\n",
        "def get_date(sentence):\n",
        "    # Use a regular expression to find a four-digit number in the sentence\n",
        "    x = re.findall(r\"\\d{4}\", sentence)\n",
        "    if x:\n",
        "        return x[0]\n",
        "    return 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJLV3gmR1Vtk"
      },
      "outputs": [],
      "source": [
        "# Define a function to extract the relation type from a template type and a sentence\n",
        "def get_relation(template_type, sentence):\n",
        "    # Depending on the template type, print the appropriate relation\n",
        "    if template_type == 'birthplace':\n",
        "        print(f\"مكان الولادة ({get_person(sentence)} , {get_location(sentence)})\")\n",
        "    if template_type == 'deathplace':\n",
        "        print(f\"مكان الوفاة ({get_person(sentence)} , {get_location(sentence)})\")\n",
        "    if template_type == 'deathdate':\n",
        "        print(f\"تاريخ الوفاة ({get_person(sentence)} , {get_date(sentence)})\")\n",
        "    if template_type == 'birthdate':\n",
        "        print(f\"تاريخ الولادة ({get_person(sentence)} , {get_date(sentence)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdUZIUPWD5mJ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# Create a copy of the external templates dictionary\n",
        "reg_templates = copy.deepcopy(ext_templates)\n",
        "\n",
        "# Loop through each template type and its list of templates in the reg_templates dictionary\n",
        "for template_type, template_list in reg_templates.items():\n",
        "    index = 0\n",
        "    # Loop through each template in the template list\n",
        "    for template in template_list:\n",
        "        # Remove all punctuation except curly braces from the template\n",
        "        punctuation_marks = r'[^\\w\\s{}]' # Define a regular expression pattern to match all punctuation except curly braces\n",
        "        template = re.sub(punctuation_marks, '', template)\n",
        "\n",
        "        # Replace all numerical placeholders with a regular expression pattern that matches any string\n",
        "        template = re.sub(r\"{\\d+}\", r\"(.+)\", template)\n",
        "\n",
        "        # Replace all word placeholders with a regular expression pattern that matches any word followed by any number of characters\n",
        "        pattern = re.compile(r\"(\\b\\w+\\b)\")\n",
        "        template = re.sub(pattern, r\"\\1(.*)\", template)\n",
        "        template = f\"(.*){template}\"\n",
        "\n",
        "        # Replace the original template in the template list with the modified version\n",
        "        reg_templates[template_type][index] = template\n",
        "        index += 1\n",
        "\n",
        "# Loop through each template type and its list of templates in the reg_templates dictionary\n",
        "for template_type, template_list in reg_templates.items():\n",
        "    index = 0\n",
        "    print(template_type)\n",
        "    for template in template_list:\n",
        "        print(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUZLJMIPNtYZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Loop through each sentence in the Data list\n",
        "cnt = 0\n",
        "for sentence in Data:\n",
        "    # Initialize a flag variable to indicate whether a relation has been found in the sentence\n",
        "    relation_found = False\n",
        "\n",
        "    for template_type, template_list in reg_templates.items():\n",
        "        # Loop through each template in the template list\n",
        "        for template in template_list:\n",
        "            # Check if the template matches the sentence using regex matching\n",
        "            if re.match(template , sentence):\n",
        "                cnt += 1\n",
        "                print(cnt)\n",
        "\n",
        "                print(sentence)\n",
        "                print(template)\n",
        "                get_relation(template_type , sentence)\n",
        "                break\n",
        "\n",
        "        if relation_found == True:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjP6RZsVEf8l"
      },
      "source": [
        "# **Results and Mistakes**\n",
        "* #### accuracy = (1029 - 93)/1029 = 90.96%\n",
        "\n",
        "* #### الأخطاء بشكل عام ناتجة عن اختيار الزاحف لجمل متوافقة مع قالب وهي أخطاء قليلة والأخطاء الأكثر هي بتعرف مكتبة فراسة على الكيانات المختلفة\n",
        "--------------------------------------------------------------------------------\n",
        "* #### The mistakes generally result from the crawler selecting sentences that match a template, which are few, and most errors are due to the Farasa library's recognition of different entities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU1WPzwsElJr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}